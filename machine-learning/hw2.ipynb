{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bryan Chen\n",
    "\n",
    "Machine Learning HW 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# train is 2004, test is 2000 election data\n",
    "names = ['obesity', 'train', 'test', 'state']\n",
    "df = pd.read_csv('obesity-election.data.txt', header=None, names=names)\n",
    "\n",
    "x_train = x_test = np.array(df['obesity'])\n",
    "y_train = np.array(df['train'])\n",
    "y_test = np.array(df['test'])\n",
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = [x for x in range(50) if x % 2 != 0]\n",
    "cv_scores = []\n",
    "\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, x_train.reshape(-1, 1), y_train, cv=LeaveOneOut(), scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "MSE = [1 - x for x in cv_scores]\n",
    "optimal_k = neighbors[cv_scores.index(max(cv_scores))]\n",
    "print('The optimal number of neighbors is', optimal_k)\n",
    "\n",
    "plt.plot(neighbors, MSE)\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of neighbors is 17. The cross validation score of the optimal number is\n",
    "0.68627 and the mean square error is 0.31373. The accuracy of the classifier is 68%. Obesity rate\n",
    "is not a good factor to predict the election result and other factors should be taken into\n",
    "consideration for a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x_train, y_train, x_test, k):\n",
    "    distances, targets = [], []\n",
    "\n",
    "    for i in range(len(x_train)):\n",
    "        distance = np.sqrt(np.sum(pow(x_test - x_train[i], 2)))\n",
    "        distances.append([distance, i])\n",
    "    distances.sort()\n",
    "\n",
    "    for i in range(k):\n",
    "        index = distances[i][1]\n",
    "        targets.append(y_train[index])\n",
    "\n",
    "    return Counter(targets).most_common(1)[0][0]\n",
    "\n",
    "\n",
    "def k_nearest_neighbor(x_train, y_train, x_test, predictions, k):\n",
    "    # training is already done in knn classifier\n",
    "    for i in range(len(x_test)):\n",
    "        predictions.append(predict(x_train=x_train, y_train=y_train, x_test=x_test[i], k=k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    k_nearest_neighbor(x_train=x_train, y_train=y_train, x_test=x_test, predictions=predictions, k=optimal_k)\n",
    "    predictions = np.asarray(predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions) * 100\n",
    "    print('The accuracy of KNN classifier is %d%%' % accuracy)\n",
    "except ValueError:\n",
    "    print('Can\\'t have more neighbors than training samples!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State obesity_rate 2004_result prediction 2000_result (GT)\n",
    "Alabama 0.301 R R R\n",
    "Alaska 0.273 R R R\n",
    "Arizona 0.233 R D R\n",
    "Arkansas 0.281 R R R\n",
    "California 0.231 D D D\n",
    "Colorado 0.21 R D R\n",
    "Connecticut 0.208 D D D\n",
    "Delaware 0.221 D D D\n",
    "D.C. 0.259 D R D\n",
    "Florida 0.233 R D R\n",
    "Georgia 0.275 R R R\n",
    "Hawaii 0.207 D D D\n",
    "Idaho 0.246 R D R\n",
    "Illinois 0.253 D R D\n",
    "Indiana 0.275 R R R\n",
    "Iowa 0.263 R R D\n",
    "Kansas 0.258 R R R\n",
    "Kentucky 0.284 R R R\n",
    "Louisiana 0.295 R R R\n",
    "Maine 0.237 D D D\n",
    "Maryland 0.252 D D D\n",
    "Massachusetts 0.209 D D D\n",
    "Michigan 0.277 D R D\n",
    "Minnesota 0.248 D D D\n",
    "Mississippi 0.344 R R R\n",
    "Missouri 0.274 R R R\n",
    "Montana 0.217 R D R\n",
    "Nebraska 0.265 R R R\n",
    "Nevada 0.236 R D R\n",
    "New Hampshire 0.236 D D R\n",
    "New Jersey 0.229 D D D\n",
    "New Mexico 0.233 R D D\n",
    "New York 0.235 D D D\n",
    "North Carolina 0.271 R R R\n",
    "North Dakota 0.259 R R R\n",
    "Ohio 0.269 R R R\n",
    "Oklahoma 0.281 R R R\n",
    "Oregon 0.25 D D D\n",
    "Pennsylvania 0.257 D R D\n",
    "Rhode Island 0.214 D D D\n",
    "South Carolina 0.292 R R R\n",
    "South Dakota 0.261 R R R\n",
    "Tennessee 0.29 R R R\n",
    "Texas 0.272 R R R\n",
    "Utah 0.218 R D R\n",
    "Vermont 0.211 D D D\n",
    "Virginia 0.252 R D R\n",
    "Washington 0.245 D D D\n",
    "West Virginia 0.306 R R R\n",
    "Wisconsin 0.255 D R D\n",
    "Wyoming 0.24 R D R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import graphviz\n",
    "\n",
    "names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "\n",
    "# loading training data\n",
    "df = pd.read_csv('iris.data.txt', header=None, names=names)\n",
    "df.head()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# create design matrix X and target vector y\n",
    "X = np.array(df.ix[:, 0:4])     # end index is exclusive\n",
    "y = np.array(df['class'])   # another way of indexing a pandas df\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The petal_width attribute was used as the first decision node of this tree generated by the\n",
    "decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'],  \n",
    "                         class_names=[\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"DecisionTree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information gain calculations:\n",
    "\n",
    "E(class) \n",
    "= P(Iris-setosa)E(Iris-setosa) + P(Iris-versicolor)E(Iris-versicolor) + P(Iris-virginica)E(Iris-\n",
    "virginica) \n",
    "= -((50/150)log_2(50/150)) * 3 \n",
    "= 1.5850\n",
    "\n",
    "E(class, pw0.8)\n",
    "= P(low)E(low) + P(high)E(high)\n",
    "= (50/150) * [-((50/50)log_2(50/50))-0-0]+ (100/150) * [0-((50/100)log_2(50/100))-\n",
    "((50/100)log_2(50/100))]\n",
    "= 0.6667\n",
    "\n",
    "G(class, pw0.8)= 1.5850-0.6667 = 0.918\n",
    "\n",
    "E(class, pw1.75)\n",
    "= P(low)E(low) + P(high)E(high)\n",
    "= (104/150) * [-((50/104)log_2(50/104))-((49/104)log_2(49/104))-((5/104)log_2(5/104))] +\n",
    "(46/150) * [0-((1/46)log_2(1/46))-((45/46)log_2(45/46))]\n",
    "= 0.8992\n",
    "\n",
    "G(class, pw1.75)= 1.5850–0.8992= 0.686\n",
    "\n",
    "E(class, pl4.95)\n",
    "= P(low)E(low) + P(high)E(high)\n",
    "= (104/150) * [-((50/104)log_2(50/104)) -((48/104)log_2(48/104)) - ((6/104)log_2(6/104))] +\n",
    "(46/150) * [0-((2/46)log_2(2/46))-((44/46)log_2(44/46))]\n",
    "= 0.9529\n",
    "\n",
    "G(class, pl4.95)= 1.5850–0.9529= 0.632\n",
    "\n",
    "E(class, pw1.65)\n",
    "= P(low)E(low) + P(high)E(high)\n",
    "= (102/150) * [-((50/102)log_2(50/102))-((48/102)log_2(48/102))-((4/102)log_2(4/102))] +\n",
    "(48/150) * [0-((2/48)log_2(2/48))-((46/48)log_2(46/48))]\n",
    "= 0.8954\n",
    "\n",
    "G(class, pw1.65)= 1.5850-0.8954= 0.690\n",
    "\n",
    "E(class, pw1.55)\n",
    "= P(low)E(low) + P(high)E(high)\n",
    "= (98/150) * [-((50/98)log_2(50/98))-((45/98)log_2(45/98))-((3/98)log_2(3/98))] + (52/150) * \n",
    "[0-((5/52)log_2(5/52))-((47/52)log_2(47/52))]\n",
    "= 0.9194\n",
    "\n",
    "G(class, pw1.55)= 1.5850-0.9194 = 0.666\n",
    "\n",
    "E(class, sl6.95)\n",
    "= P(low)E(low) + P(high)E(high)\n",
    "= (137/150) * [-((50/137)log_2(49/137))-((49/137)log_2(49/137))-((38/137)log_2(38/137))] +\n",
    "(13/150) * [0-((1/13)log_2(1/13))-((12/13)log_2(12/13))]\n",
    "= 1.4816\n",
    "\n",
    "G(class, sl6.95)= 1.5850-1.4816= 0.103\n",
    "\n",
    "E(class, pl4.85)\n",
    "= P(low)E(low) + P(high)E(high)\n",
    "= (99/150) * [-((50/99)log_2(50/99))-((46/99)log_2(46/99))-((3/99)log_2(3/99))] + (51/150) *\n",
    "[0-((4/51)log_2(4/51))-((47/51)log_2(47/51))]\n",
    "= 0.9034\n",
    "\n",
    "G(class, pl4.85)= 1.5850-0.9034 = 0.6816\n",
    "\n",
    "E(class, pl5.95)\n",
    "= P(low)E(low) + P(high)E(high)\n",
    "= (83/150) * [-((50/83)log_2(50/83))-((26/83)log_2(26/83))-((7/83)log_2(7/83))] + (67/150) *\n",
    "[0-((24/67)log_2(24/67))-((43/67)log_2(43/67))]\n",
    "= 1.1209\n",
    "\n",
    "G(class, pl5.95)= 1.5850-1.1209 = 0.4641\n",
    "\n",
    "The best classification is the first classification, which is to see whether petal_width is smaller\n",
    "than 0.8 and the gain is 0.918, which is very close to 1, which is the highest among the 8\n",
    "calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import polyfit, polyval\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import log\n",
    "\n",
    "x = np.random.uniform(low=0, high=1, size=10)\n",
    "noise =  np.random.normal(loc=0, scale=0.5, size=10)\n",
    "y = [pow(x[i], 2) + 0.1 * x[i] + noise[i] for i in range(10)]\n",
    "\n",
    "import numpy as np\n",
    "from numpy import polyfit\n",
    "from numpy import polyval\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import log\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "X = np.random.uniform(0,1,10)\n",
    "y = np.zeros(shape=(10,))\n",
    "for i in range(len(X)):    \n",
    "    mu = 0\n",
    "    sigma = 0.5\n",
    "    theta = np.random.normal(mu, sigma, 1)   # another way of indexing a pandas df\n",
    "    y[i] = np.square(X[i]) + 0.1 * X[i] + theta\n",
    "    \n",
    "f = open('problem4.csv','w')\n",
    "f.write('m'+','+'DoF'+','+'Remp'+','+'r'+','+'Rpen'+'\\r\\n')\n",
    "for i in range(9):\n",
    "    pi = polyfit(X,y,i)\n",
    "    MSE = mean_squared_error(y, polyval(pi,X))\n",
    "    p = MSE / len(X)\n",
    "    r = 1 + p * (1-p) ** -1 * log(len(X))\n",
    "    Rpen = r * MSE\n",
    "  #  title = \"DoF = \"+ str(i)\n",
    "    f.write(str(i)+','+str(i+1)+','+'%.3f'%MSE+','+\n",
    "                   '%.3f'%r+','+'%.3f'%Rpen+'\\r\\n')\n",
    "    pi = np.poly1d(pi)\n",
    "    xp = np.linspace(0, 1, 100)\n",
    "    plt.title(\"DoF = \"+ str(i))\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Outputs')\n",
    "    plt.plot(X, y, \"o\")\n",
    "    _ = plt.plot(xp, pi(xp), '-')\n",
    "    plt.figure(i+1)\n",
    "    plt.show()\n",
    "f.close()\n",
    "\n",
    "print (pd.read_csv('problem4.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "kf = KFold(n_splits = 5)\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "optim_data = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(x):\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    for m in range(0,7):\n",
    "        error = 0\n",
    "        for train_index,validation_index in loo.split(x_train):\n",
    "            x_trn, x_val = x_train[train_index], x_train[validation_index]\n",
    "            y_trn, y_val = y_train[train_index], y_train[validation_index]\n",
    "            coefficient = np.polyfit(x_trn,y_trn,m)\n",
    "            error += abs(y_val[0] - poly_predict(x_val[0],coefficient))\n",
    "        if m == 0:\n",
    "            min_error = error\n",
    "            best_m = m\n",
    "            best_model = coefficient\n",
    "        if error < min_error:\n",
    "            best_m = m\n",
    "            best_model = coefficient\n",
    "    test_error = np.divide(np.sum(np.square(poly_predict(x_test,best_model) - y_test)),len(x_test))\n",
    "    optim_data.append([test_idx,best_m,test_error])\n",
    "\n",
    "file_p5 = open('problem5.csv','w')\n",
    "file_p5.write('Fold #'+','+'Optimal m'+','+'Prediction accuracy'+'\\r\\n')\n",
    "for (idx,item) in enumerate(optim_data):\n",
    "    file_p5.write(str(idx)+','+str(item[1])+','+'%.3f'%item[2]+'\\r\\n')\n",
    "file_p5.close()\n",
    "\n",
    "print(pd.read_csv('problem5.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
